集合数据源 -> 分区设定

~~~scala
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
sparkConf.set("spark.default.parallelism", "5")
val sc = new SparkContext(sparkConf)

// RDD的并行度 & 分区
// makeRDD方法可以传递第二个参数, 这个参数表示分区的数量
// 第二个参数可以不传递, 那么makeRDD方法会使用默认值: defaultParallelism
// scheduler.conf.getInt("spark.default.parallelism", "totalCores")
// spark在默认情况下, 从配置对象中获取配置参数: spark.default.parallelism
// 如果获取不到, 那么使用totalCores属性, 这个属性值为当前运行环境的最大可用核数

val rdd =  sc.makeRDD(List(1,2,3,4), 2)
val rdd = sc.makeRDD(List(1,2,3,4))

//将处理的数据保存成分区文件
rdd.saveAsTextFile("output")

~~~



集合数据源->分区数据的分配

~~~scala

val rdd = sc.makeRDD(List(1,2,3,4), 2)
// [1,2] [3,4]

val rdd = sc.makeRDD(List(1,2,3,4), 3)
// [1] [2] [3,4]

val rdd = sc.makeRDD(List(1,2,3,4,5), 3)
// [1] [2,3] [4,5]

rdd.saveAsTextFile("output")

~~~



分区源码

~~~scala
def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {
	(0 until numSlices).iterator.map { i =>
        val start = ((i * length) / numSlices).toInt
        val end = (((i + 1) * length) / numSlices).toInt
        (start, end)
    }
}

0 => (0, 1)
1 => (1, 3)
~~~



文件数据源 -> 分区的设定

~~~scala
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
val sc = new SparkContext(sparkConf)

// textFile可以将文件作为数据处理的数据源, 默认也可以设定分区
// minPartitions : 最小分区数量
// math.min(defaultParallelism, 2)
val rdd = sc.textFile("datas/1.txt")
//如果不想使用默认的分区数量, 可以通过第二个参数指定分区数
// Spark读取文件, 底层其实使用的就是Hadoop的读取方式
// 分区数量的计算方式
// totalSize = 7
// goalSize = 7 / 2 = 3(byte)
// 7 / 3 = 2...1 (1.1) + 1 = 3(分区)

val rdd = sc.textFile("datas/1.txt", 2)
rdd.saveAsFile("output")
~~~



文件数据源->分区数据的分配

~~~scala
// spark读取文件, 采用的是hadoop的方式读取, 所以一行一行读取, 和字节数没有关系
// 数据读取时以偏移量为单位, 偏移量不会被重复读取

// 1@@ => 012
// 2@@ => 345
// 3   => 6

//数据分区的偏移量范围的计算
// 0 => [0, 3] => 12
// 1 => [3, 6] => 3
// 3 => [6, 7] =>

val rdd = sc.textFile("datas/1.txt", 2)
~~~



